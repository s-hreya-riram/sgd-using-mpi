{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c9baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# CONSTANTS\n",
    "# --------------------\n",
    "# Define expected input columns\n",
    "EXPECTED_INPUT_COLUMNS = [\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RatecodeID\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"extra\",\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "# Columns after one-hot encoding RatecodeID and payment_type\n",
    "EXPECTED_RATECODE_COLUMNS = [f\"RatecodeID_{i}\" for i in [1, 2, 3, 4, 5, 6, 99]]\n",
    "EXPECTED_PAYMENT_COLUMNS = [f\"payment_type_{i}\" for i in [1, 2, 3, 4, 5]]\n",
    "\n",
    "\n",
    "# Columns associated with datetime features\n",
    "EXPECTED_DATETIME_FEATURES = [\n",
    "    f\"tpep_pickup_datetime_{unit}\" for unit in [\"day\", \"month\", \"year\", \"hour\", \"minute\", \"second\"]\n",
    "] + [\n",
    "    f\"tpep_dropoff_datetime_{unit}\" for unit in [\"day\", \"month\", \"year\", \"hour\", \"minute\", \"second\"]\n",
    "]\n",
    "\n",
    "# Complete list of features (excluding label)\n",
    "EXPECTED_SCHEMA = (\n",
    "    [\"passenger_count\", \"trip_distance\", \"extra\", \"PULocationID\", \"DOLocationID\"] \n",
    "    + EXPECTED_RATECODE_COLUMNS\n",
    "    + EXPECTED_PAYMENT_COLUMNS\n",
    "    + EXPECTED_DATETIME_FEATURES\n",
    "    + [\"trip_duration\"]\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# FUNCTIONS\n",
    "# --------------------\n",
    "\n",
    "def preprocess_chunk(df):\n",
    "    \"\"\"\n",
    "    Preprocess one chunk of the taxi dataset.\n",
    "    - narrowing down to the columns mentioned in the problem statement\n",
    "    - drop NAs and filter invalid data\n",
    "    - add encoding for categorical variables (one-hot or frequency encoding depending on frequency)\n",
    "    Returns (X, y).\n",
    "    \"\"\"\n",
    "    df = df[EXPECTED_INPUT_COLUMNS].copy()\n",
    "    \n",
    "    # Drop rows with any NA values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Clean extra and total_amount values to be non-negative and positive respectively \n",
    "    # based on the attribute descriptions from Kaggle: https://www.kaggle.com/datasets/diishasiing/revenue-for-cab-drivers/data\n",
    "    df = df[df[\"extra\"] >= 0]\n",
    "    df = df[df[\"total_amount\"] > 0]\n",
    "\n",
    "    # Convert datetime columns (vectorized)\n",
    "    df.loc[:, \"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "    df.loc[:, \"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Derive datetime features\n",
    "    df = get_datetime_features(df, \"tpep_pickup_datetime\")\n",
    "    df = get_datetime_features(df, \"tpep_dropoff_datetime\")\n",
    "\n",
    "    # Trip duration in minutes\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60\n",
    "\n",
    "    # Drop original datetime cols\n",
    "    df.drop(columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"], inplace=True)\n",
    "\n",
    "    # Since there are 6 unique values for RatecodeID, 263 for PULocationID, 262 for DOLocationID and 5 for payment_type\n",
    "    # taking into account the volume of data, using one hot encoding for ratecodeId and payment_type,\n",
    "    # using frequency encoding for PULocationID and DOLocationID\n",
    "\n",
    "    # One-hot encoding RatecodeID and payment_type, having to manually specify expected columns to address the issue from \n",
    "    # testing - some chunks may not have all categories, that was causing inconsistent number of columns\n",
    "    # across chunks and causing errors like \"ValueError: all the input array dimensions except for the concatenation axis\n",
    "    # must match exactly, but along dimension 1, the array at index 0 has size 28 and the array at index 1 has size 29\"\n",
    "    # hardcoded values are from the exploratory data analysis notebook\n",
    "    # TODO explore refactoring this to avoid hardcoding\n",
    "    df = pd.get_dummies(df, columns=[\"RatecodeID\", \"payment_type\"], prefix=[\"RatecodeID\", \"payment_type\"])\n",
    "\n",
    "    for col in [\"PULocationID\", \"DOLocationID\"]:\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[col] = df[col].map(freq)\n",
    "\n",
    "    # Add missing dummy columns with 0s\n",
    "    for col in EXPECTED_RATECODE_COLUMNS + EXPECTED_PAYMENT_COLUMNS:\n",
    "        if col not in df:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Keep column order consistent\n",
    "    df = df.reindex(columns=EXPECTED_SCHEMA + [\"total_amount\"], fill_value=0)\n",
    "\n",
    "    # Tracking feature_columns and skip_normalization_columns to skip normalization of the attributes\n",
    "    # that are the derived date-time attributes, were one-hot encoded or frequency encoded above\n",
    "    # TODO see if there is an alternative to manually specifying the column names\n",
    "    skip_normalization_columns = [\n",
    "        col for col in df.columns\n",
    "        if col.startswith(\"RatecodeID_\")\n",
    "        or col.startswith(\"payment_type_\")\n",
    "        or col.startswith(\"tpep_pickup_datetime_\")\n",
    "        or col.startswith(\"tpep_dropoff_datetime_\")\n",
    "        or col in [\"PULocationID\", \"DOLocationID\"]\n",
    "    ]\n",
    "    feature_columns = [c for c in df.columns if c != \"total_amount\"]\n",
    "\n",
    "    # ensuring X and y are of type float64 as object type arrays cause errors with MPI Allreduce\n",
    "    X = df[feature_columns].values.astype(np.float64) \n",
    "    y = df[\"total_amount\"].values.astype(np.float64)\n",
    "\n",
    "    # stack X and y back into a dataframe to return a single dataframe\n",
    "    df = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))), columns=feature_columns + [\"total_amount\"])\n",
    "    return df \n",
    "\n",
    "def get_datetime_features(df, col_name):\n",
    "    '''\n",
    "    Derive datetime features from a datetime column\n",
    "    '''\n",
    "    dt = df[col_name].dt\n",
    "    features = pd.DataFrame({\n",
    "        col_name + \"_day\": dt.day,\n",
    "        col_name + \"_month\": dt.month,\n",
    "        col_name + \"_year\": dt.year,\n",
    "        col_name + \"_hour\": dt.hour,\n",
    "        col_name + \"_minute\": dt.minute,\n",
    "        col_name + \"_second\": dt.second,\n",
    "    }, index=df.index)\n",
    "    return pd.concat([df, features], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/c0cfqj715njfdnphhzw8m5p00000gn/T/ipykernel_96530/2450865496.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../../data/nytaxi2022.csv\", header=0)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/nytaxi2022.csv\", header=0, parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yk/c0cfqj715njfdnphhzw8m5p00000gn/T/ipykernel_96530/1112001045.py:65: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.loc[:, \"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_chunk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_csv(\"../../data/nytaxi2022_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84859e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
