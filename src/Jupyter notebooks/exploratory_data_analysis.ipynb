{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff48467",
   "metadata": {},
   "source": [
    "### Jupyter Notebook to clean up the nytaxi2022.csv dataset\n",
    "\n",
    "The purpose of this notebook is to do basic exploratory data analysis to check for missing/invalid data, remove outliers, perform encoding for categorical variables, prior to execution of Stochastic Gradient Descent using MPI.\n",
    "The description of the attributes from [Kaggle](https://www.kaggle.com/datasets/diishasiing/revenue-for-cab-drivers/data) is as follows:\n",
    "\n",
    "* VendorID: A unique identifier for the taxi vendor or service provider.\n",
    "* tpep_pickup_datetime: The date and time when the passenger was picked up.\n",
    "* tpep_dropoff_datetime: The date and time when the passenger was dropped off.\n",
    "* passenger_count: The number of passengers in the taxi.\n",
    "* trip_distance: The total distance of the trip in miles or kilometers.\n",
    "* RatecodeID: The rate code assigned to the trip, representing fare types.\n",
    "* store_and_fwd_flag: Indicates whether the trip data was stored locally and then forwarded later (Y/N).\n",
    "* PULocationID: The unique identifier for the pickup location (zone or area).\n",
    "* DOLocationID: The unique identifier for the drop-off location (zone or area).\n",
    "* payment_type: The method of payment used by the passenger (e.g., cash, card).\n",
    "* fare_amount: The base fare for the trip.\n",
    "* extra: Additional charges applied during the trip (e.g., night surcharge).\n",
    "* mta_tax: The tax imposed by the Metropolitan Transportation Authority.\n",
    "* tip_amount: The tip given to the driver, if applicable.\n",
    "* tolls_amount: The total amount of tolls charged during the trip.\n",
    "* improvement_surcharge: A surcharge imposed for the improvement of services.\n",
    "* total_amount: The total fare amount, including all charges and surcharges.\n",
    "* congestion_surcharge: An additional charge for trips taken during high traffic congestion times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b921a3",
   "metadata": {},
   "source": [
    "Below we import the necessary libraries and define some of the constants of relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a324400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# CONSTANTS\n",
    "# --------------------\n",
    "# Define expected input columns\n",
    "EXPECTED_INPUT_COLUMNS = [\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RatecodeID\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"extra\",\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "# Columns after one-hot encoding RatecodeID and payment_type\n",
    "EXPECTED_RATECODE_COLUMNS = [f\"RatecodeID_{i}\" for i in [1, 2, 3, 4, 5, 6, 99]]\n",
    "EXPECTED_PAYMENT_COLUMNS = [f\"payment_type_{i}\" for i in [1, 2, 3, 4, 5]]\n",
    "\n",
    "\n",
    "# Columns associated with datetime features\n",
    "EXPECTED_DATETIME_FEATURES = [\n",
    "    f\"tpep_pickup_datetime_{unit}\" for unit in [\"day\", \"month\", \"year\", \"hour\", \"minute\", \"second\"]\n",
    "] + [\n",
    "    f\"tpep_dropoff_datetime_{unit}\" for unit in [\"day\", \"month\", \"year\", \"hour\", \"minute\", \"second\"]\n",
    "]\n",
    "\n",
    "# Complete list of features (excluding label)\n",
    "EXPECTED_FEATURE_COLUMNS = (\n",
    "    [\"passenger_count\", \"trip_distance\", \"extra\", \"PULocationID\", \"DOLocationID\"] \n",
    "    + EXPECTED_RATECODE_COLUMNS\n",
    "    + EXPECTED_PAYMENT_COLUMNS\n",
    "    + EXPECTED_DATETIME_FEATURES\n",
    "    + [\"trip_duration\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb53321",
   "metadata": {},
   "source": [
    "To avoid having to read the Python file multiple times, storing the pickled file for faster reads and processing in subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bee305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_file = \"../../data/nytaxi2022.csv\"\n",
    "pkl_file = \"../../data/nytaxi2022.pkl\"\n",
    "\n",
    "if os.path.exists(pkl_file):\n",
    "    # Fast path: load from pickle\n",
    "    df = pd.read_pickle(pkl_file)\n",
    "else:\n",
    "    # Slow path: read CSV, then cache as pickle\n",
    "    df = pd.read_csv(\n",
    "        csv_file,\n",
    "        header=0,\n",
    "        parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "    )\n",
    "    df.to_pickle(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcfbc1",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e027d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before any exploration, limit the dataframe to only the expected input columns\n",
    "df = df[EXPECTED_INPUT_COLUMNS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc9a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "tpep_pickup_datetime     0.000000\n",
      "tpep_dropoff_datetime    0.000000\n",
      "passenger_count          3.450423\n",
      "trip_distance            0.000000\n",
      "RatecodeID               3.450423\n",
      "PULocationID             0.000000\n",
      "DOLocationID             0.000000\n",
      "payment_type             0.000000\n",
      "extra                    0.000000\n",
      "total_amount             0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values/len(df) * 100)  # Print as percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6a219",
   "metadata": {},
   "source": [
    "Since the percentage of missing values isn't significantly high, it is alright to drop the corresponding rows rather than imputing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7861aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0801a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>extra</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38287795</td>\n",
       "      <td>38287795</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "      <td>3.828780e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-07-05 00:02:30.606806528</td>\n",
       "      <td>2022-07-05 00:57:11.752203264</td>\n",
       "      <td>1.401149e+00</td>\n",
       "      <td>3.514399e+00</td>\n",
       "      <td>1.424172e+00</td>\n",
       "      <td>1.649293e+02</td>\n",
       "      <td>1.628563e+02</td>\n",
       "      <td>1.232061e+00</td>\n",
       "      <td>1.040985e+00</td>\n",
       "      <td>2.142373e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2001-01-01 00:03:14</td>\n",
       "      <td>2001-01-01 00:34:17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.218000e+01</td>\n",
       "      <td>-2.567800e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-04-07 19:04:36.500000</td>\n",
       "      <td>2022-04-07 19:21:25</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.100000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.320000e+02</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.230000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-06-30 23:33:04</td>\n",
       "      <td>2022-06-30 23:50:20</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.860000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.620000e+02</td>\n",
       "      <td>1.620000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.595000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-10-04 17:04:52.500000</td>\n",
       "      <td>2022-10-04 17:23:22</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.490000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.340000e+02</td>\n",
       "      <td>2.340000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>2.277000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-04-18 14:30:05</td>\n",
       "      <td>2023-04-18 23:30:39</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.843408e+05</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.650000e+02</td>\n",
       "      <td>2.650000e+02</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>3.350000e+01</td>\n",
       "      <td>4.010956e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.628938e-01</td>\n",
       "      <td>5.643246e+01</td>\n",
       "      <td>5.794343e+00</td>\n",
       "      <td>6.494462e+01</td>\n",
       "      <td>7.015708e+01</td>\n",
       "      <td>4.760829e-01</td>\n",
       "      <td>1.271069e+00</td>\n",
       "      <td>9.801355e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tpep_pickup_datetime          tpep_dropoff_datetime  \\\n",
       "count                       38287795                       38287795   \n",
       "mean   2022-07-05 00:02:30.606806528  2022-07-05 00:57:11.752203264   \n",
       "min              2001-01-01 00:03:14            2001-01-01 00:34:17   \n",
       "25%       2022-04-07 19:04:36.500000            2022-04-07 19:21:25   \n",
       "50%              2022-06-30 23:33:04            2022-06-30 23:50:20   \n",
       "75%       2022-10-04 17:04:52.500000            2022-10-04 17:23:22   \n",
       "max              2023-04-18 14:30:05            2023-04-18 23:30:39   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "       passenger_count  trip_distance    RatecodeID  PULocationID  \\\n",
       "count     3.828780e+07   3.828780e+07  3.828780e+07  3.828780e+07   \n",
       "mean      1.401149e+00   3.514399e+00  1.424172e+00  1.649293e+02   \n",
       "min       0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00   \n",
       "25%       1.000000e+00   1.100000e+00  1.000000e+00  1.320000e+02   \n",
       "50%       1.000000e+00   1.860000e+00  1.000000e+00  1.620000e+02   \n",
       "75%       1.000000e+00   3.490000e+00  1.000000e+00  2.340000e+02   \n",
       "max       9.000000e+00   1.843408e+05  9.900000e+01  2.650000e+02   \n",
       "std       9.628938e-01   5.643246e+01  5.794343e+00  6.494462e+01   \n",
       "\n",
       "       DOLocationID  payment_type         extra  total_amount  \n",
       "count  3.828780e+07  3.828780e+07  3.828780e+07  3.828780e+07  \n",
       "mean   1.628563e+02  1.232061e+00  1.040985e+00  2.142373e+01  \n",
       "min    1.000000e+00  1.000000e+00 -2.218000e+01 -2.567800e+03  \n",
       "25%    1.130000e+02  1.000000e+00  0.000000e+00  1.230000e+01  \n",
       "50%    1.620000e+02  1.000000e+00  5.000000e-01  1.595000e+01  \n",
       "75%    2.340000e+02  1.000000e+00  2.500000e+00  2.277000e+01  \n",
       "max    2.650000e+02  5.000000e+00  3.350000e+01  4.010956e+05  \n",
       "std    7.015708e+01  4.760829e-01  1.271069e+00  9.801355e+01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a desctriptive statistics summary of the dataframe\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf09a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 1\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate rows so they don't skew the model training\n",
    "duplicate_rows = df.duplicated()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938169e",
   "metadata": {},
   "source": [
    "Since there is only 1 duplicate row, we can drop it without much concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deab46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfbe79",
   "metadata": {},
   "source": [
    "The descriptive statistics along with the Kaggle description gives a good idea of the distribution of the numerical attributes at hand.\n",
    "We will perform the following in subsequent steps:\n",
    "- Modify the datetime objects to get the individual features (day, month, year, hour, minute, second) and defining the trip duration attribute, dropping the original datetime attributes \n",
    "- Describe the trip duration attribute to potentially filter out any negative or extremely large values\n",
    "- Look for outliers for extra and total_amount as well\n",
    "- Identify unique values for the categorical columns (RatecodeID, PULocationID, DOLocationID, payment_type) to determine the encoding that may be suitable for them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f5dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_features(df, col_name):\n",
    "    '''\n",
    "    Derive datetime features from a datetime column\n",
    "    '''\n",
    "    dt = df[col_name].dt\n",
    "    features = pd.DataFrame({\n",
    "        col_name + \"_day\": dt.day,\n",
    "        col_name + \"_month\": dt.month,\n",
    "        col_name + \"_year\": dt.year,\n",
    "        col_name + \"_hour\": dt.hour,\n",
    "        col_name + \"_minute\": dt.minute,\n",
    "        col_name + \"_second\": dt.second,\n",
    "    }, index=df.index)\n",
    "    return pd.concat([df, features], axis=1)\n",
    "\n",
    "# Derive datetime features\n",
    "df = get_datetime_features(df, \"tpep_pickup_datetime\")\n",
    "df = get_datetime_features(df, \"tpep_dropoff_datetime\")\n",
    "\n",
    "# Trip duration in minutes\n",
    "df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60\n",
    "\n",
    "# Drop original datetime columns\n",
    "df.drop(columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4af6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.828779e+07\n",
       "mean     5.468576e+01\n",
       "std      1.966705e+04\n",
       "min     -7.175993e+04\n",
       "25%      7.350000e+00\n",
       "50%      1.213333e+01\n",
       "75%      1.966667e+01\n",
       "max      1.032206e+07\n",
       "Name: trip_duration, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the trip duration to identify any negative or extremely large values\n",
    "df[\"trip_duration\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c708859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop trips with negative or zero duration, limit to trips less than 180 minutes (3 hours)\n",
    "df = df[(df[\"trip_duration\"] > 0) & (df[\"trip_duration\"] <= 180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db84bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extra</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.822030e+07</td>\n",
       "      <td>3.822030e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.041380e+00</td>\n",
       "      <td>2.141236e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.271175e+00</td>\n",
       "      <td>9.806654e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.218000e+01</td>\n",
       "      <td>-2.567800e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.230000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.595000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>2.277000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.350000e+01</td>\n",
       "      <td>4.010956e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              extra  total_amount\n",
       "count  3.822030e+07  3.822030e+07\n",
       "mean   1.041380e+00  2.141236e+01\n",
       "std    1.271175e+00  9.806654e+01\n",
       "min   -2.218000e+01 -2.567800e+03\n",
       "25%    0.000000e+00  1.230000e+01\n",
       "50%    5.000000e-01  1.595000e+01\n",
       "75%    2.500000e+00  2.277000e+01\n",
       "max    3.350000e+01  4.010956e+05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for outliers in extra and total_amount\n",
    "df[['extra', 'total_amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aab571",
   "metadata": {},
   "source": [
    "Going by the description of attributes from Kaggle, both the attributes cannot have negative values, with extra being non-negative and total_amount being positive. Also, the max value of total_amount is extremely high compared to the mean, we can try looking at some confidence intervals to see the rows we want to retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound: 5.8, Upper bound: 93.96\n",
      "Percentage of outliers: 0.9394557120998955\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows with negative extra and zero/negative total_amount\n",
    "df = df[(df[\"extra\"] >= 0) & (df[\"total_amount\"] > 0)]\n",
    "\n",
    "# Identify 99% confidence interval for total_amount to filter out extreme outliers\n",
    "lower_bound = df[\"total_amount\"].quantile(0.005)\n",
    "upper_bound = df[\"total_amount\"].quantile(0.995)\n",
    "print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
    "print(f\"Percentage of outliers: {len(df[(df['total_amount'] < lower_bound) | (df['total_amount'] > upper_bound)])/len(df) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa9587",
   "metadata": {},
   "source": [
    "As the percentage of outliers looks admissible, we will proceed to drop the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5a00a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['total_amount'] >= lower_bound) & (df['total_amount'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378f46ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for RatecodeID:\n",
      "RatecodeID\n",
      "1.0     94.978401\n",
      "2.0      3.857667\n",
      "5.0      0.601269\n",
      "99.0     0.350761\n",
      "3.0      0.128166\n",
      "4.0      0.083433\n",
      "6.0      0.000303\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Value counts for payment_type:\n",
      "payment_type\n",
      "1    79.372954\n",
      "2    20.062210\n",
      "3     0.320594\n",
      "4     0.244240\n",
      "5     0.000003\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Value counts for PULocationID:\n",
      "PULocationID\n",
      "237    4.814447\n",
      "132    4.767549\n",
      "236    4.258112\n",
      "161    4.057440\n",
      "186    3.361516\n",
      "         ...   \n",
      "109    0.000035\n",
      "199    0.000032\n",
      "204    0.000024\n",
      "84     0.000016\n",
      "99     0.000011\n",
      "Name: proportion, Length: 262, dtype: float64\n",
      "\n",
      "Value counts for DOLocationID:\n",
      "DOLocationID\n",
      "236    4.362336\n",
      "237    4.164382\n",
      "161    3.710522\n",
      "170    2.998097\n",
      "230    2.987994\n",
      "         ...   \n",
      "2      0.000096\n",
      "204    0.000074\n",
      "105    0.000056\n",
      "99     0.000045\n",
      "110    0.000003\n",
      "Name: proportion, Length: 262, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather the frequency counts of categorical variables\n",
    "categorical_columns = [\"RatecodeID\", \"payment_type\", \"PULocationID\", \"DOLocationID\"]\n",
    "for col in categorical_columns:\n",
    "    print(f\"Value counts for {col}:\")\n",
    "    print(df[col].value_counts(normalize=True) * 100)  # Print as percentage\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd4f78",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211193af",
   "metadata": {},
   "source": [
    "Since the RatecodeID and payment_type have few dominant categories, we will proceed with one-hot encoding them. The PULocationID and DOLocationID have a large number of unique values, so we will perform frequency encoding on them.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75354b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Since there are 6 unique values for RatecodeID, 263 for PULocationID, 262 for DOLocationID and 5 for payment_type\n",
    "    # taking into account the volume of data, using one hot encoding for ratecodeId and payment_type,\n",
    "    # using frequency encoding for PULocationID and DOLocationID\n",
    "    df[\"RatecodeID\"] = df[\"RatecodeID\"].astype(\"Int64\")\n",
    "    df[\"RatecodeID\"] = pd.Categorical(df[\"RatecodeID\"], categories=[1, 2, 3, 4, 5, 6, 99])\n",
    "\n",
    "    df[\"payment_type\"] = df[\"payment_type\"].astype(\"Int64\")\n",
    "    df[\"payment_type\"] = pd.Categorical(df[\"payment_type\"], categories=[1, 2, 3, 4, 5])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"RatecodeID\", \"payment_type\"], prefix=[\"RatecodeID\", \"payment_type\"])\n",
    "\n",
    "    for col in [\"PULocationID\", \"DOLocationID\"]:\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[col] = df[col].map(freq).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8be6d2",
   "metadata": {},
   "source": [
    "### Final round of checks before publishing the processed file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c6711",
   "metadata": {},
   "source": [
    "Below we do some final checks before writing the data post processing data to a file to use in the main Python code for NeuralNet implementation of SGD using MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0970b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_checks(df):\n",
    "    # Tracking feature_columns and skip_normalization_columns to skip normalization of the attributes\n",
    "    # that are the derived date-time attributes, were one-hot encoded or frequency encoded above\n",
    "    # these features are for exploring the variation of rmse with and without normalization\n",
    "    # of categorical features for a neural net using SGD\n",
    "    skip_normalization_columns = [\n",
    "        col for col in df.columns\n",
    "        if col.startswith(\"RatecodeID_\")\n",
    "        or col.startswith(\"payment_type_\")\n",
    "        or col.startswith(\"tpep_pickup_datetime_\")\n",
    "        or col.startswith(\"tpep_dropoff_datetime_\")\n",
    "        or col in [\"PULocationID\", \"DOLocationID\"]\n",
    "    ]\n",
    "    \n",
    "    feature_columns = [c for c in df.columns if c != \"total_amount\"]\n",
    "\n",
    "    # ensuring X and y are of type float64 as object type arrays cause errors with MPI Allreduce\n",
    "    X = df[feature_columns].values.astype(np.float64) \n",
    "    y = df[\"total_amount\"].values.astype(np.float64)\n",
    "\n",
    "    # stack X and y back into a dataframe to return a single dataframe\n",
    "    df = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))), columns=feature_columns + [\"total_amount\"])\n",
    "    return df, feature_columns, skip_normalization_columns\n",
    "\n",
    "df, feature_columns, skip_normalization_columns = final_checks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fab015d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>extra</th>\n",
       "      <th>tpep_pickup_datetime_day</th>\n",
       "      <th>tpep_pickup_datetime_month</th>\n",
       "      <th>tpep_pickup_datetime_year</th>\n",
       "      <th>tpep_pickup_datetime_hour</th>\n",
       "      <th>tpep_pickup_datetime_minute</th>\n",
       "      <th>...</th>\n",
       "      <th>RatecodeID_4</th>\n",
       "      <th>RatecodeID_5</th>\n",
       "      <th>RatecodeID_6</th>\n",
       "      <th>RatecodeID_99</th>\n",
       "      <th>payment_type_1</th>\n",
       "      <th>payment_type_2</th>\n",
       "      <th>payment_type_3</th>\n",
       "      <th>payment_type_4</th>\n",
       "      <th>payment_type_5</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "      <td>3.760366e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.400886e+00</td>\n",
       "      <td>3.449259e+00</td>\n",
       "      <td>2.572680e-02</td>\n",
       "      <td>2.014759e-02</td>\n",
       "      <td>1.057177e+00</td>\n",
       "      <td>1.561934e+01</td>\n",
       "      <td>6.602548e+00</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.421867e+01</td>\n",
       "      <td>2.953681e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.343337e-04</td>\n",
       "      <td>6.012686e-03</td>\n",
       "      <td>3.031620e-06</td>\n",
       "      <td>3.507611e-03</td>\n",
       "      <td>7.937295e-01</td>\n",
       "      <td>2.006221e-01</td>\n",
       "      <td>3.205938e-03</td>\n",
       "      <td>2.442395e-03</td>\n",
       "      <td>2.659316e-08</td>\n",
       "      <td>2.118878e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.632015e-01</td>\n",
       "      <td>5.374978e+01</td>\n",
       "      <td>1.238989e-02</td>\n",
       "      <td>1.147161e-02</td>\n",
       "      <td>1.269129e+00</td>\n",
       "      <td>8.750457e+00</td>\n",
       "      <td>3.374099e+00</td>\n",
       "      <td>5.610413e-02</td>\n",
       "      <td>5.737698e+00</td>\n",
       "      <td>1.732159e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.887278e-02</td>\n",
       "      <td>7.730805e-02</td>\n",
       "      <td>1.741152e-03</td>\n",
       "      <td>5.912113e-02</td>\n",
       "      <td>4.046269e-01</td>\n",
       "      <td>4.004658e-01</td>\n",
       "      <td>5.653017e-02</td>\n",
       "      <td>4.936021e-02</td>\n",
       "      <td>1.630741e-04</td>\n",
       "      <td>1.534797e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.063726e-07</td>\n",
       "      <td>2.659316e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.001000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.800000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.100000e+00</td>\n",
       "      <td>1.660628e-02</td>\n",
       "      <td>1.195596e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.230000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.870000e+00</td>\n",
       "      <td>2.644139e-02</td>\n",
       "      <td>2.112946e-02</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.596000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.470000e+00</td>\n",
       "      <td>3.329522e-02</td>\n",
       "      <td>2.743901e-02</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2.022000e+03</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.256000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.843408e+05</td>\n",
       "      <td>4.814447e-02</td>\n",
       "      <td>4.362336e-02</td>\n",
       "      <td>3.350000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.023000e+03</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>5.900000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.396000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       "count     3.760366e+07   3.760366e+07  3.760366e+07  3.760366e+07   \n",
       "mean      1.400886e+00   3.449259e+00  2.572680e-02  2.014759e-02   \n",
       "std       9.632015e-01   5.374978e+01  1.238989e-02  1.147161e-02   \n",
       "min       0.000000e+00   0.000000e+00  1.063726e-07  2.659316e-08   \n",
       "25%       1.000000e+00   1.100000e+00  1.660628e-02  1.195596e-02   \n",
       "50%       1.000000e+00   1.870000e+00  2.644139e-02  2.112946e-02   \n",
       "75%       1.000000e+00   3.470000e+00  3.329522e-02  2.743901e-02   \n",
       "max       9.000000e+00   1.843408e+05  4.814447e-02  4.362336e-02   \n",
       "\n",
       "              extra  tpep_pickup_datetime_day  tpep_pickup_datetime_month  \\\n",
       "count  3.760366e+07              3.760366e+07                3.760366e+07   \n",
       "mean   1.057177e+00              1.561934e+01                6.602548e+00   \n",
       "std    1.269129e+00              8.750457e+00                3.374099e+00   \n",
       "min    0.000000e+00              1.000000e+00                1.000000e+00   \n",
       "25%    0.000000e+00              8.000000e+00                4.000000e+00   \n",
       "50%    5.000000e-01              1.500000e+01                6.000000e+00   \n",
       "75%    2.500000e+00              2.300000e+01                1.000000e+01   \n",
       "max    3.350000e+01              3.100000e+01                1.200000e+01   \n",
       "\n",
       "       tpep_pickup_datetime_year  tpep_pickup_datetime_hour  \\\n",
       "count               3.760366e+07               3.760366e+07   \n",
       "mean                2.022000e+03               1.421867e+01   \n",
       "std                 5.610413e-02               5.737698e+00   \n",
       "min                 2.001000e+03               0.000000e+00   \n",
       "25%                 2.022000e+03               1.100000e+01   \n",
       "50%                 2.022000e+03               1.500000e+01   \n",
       "75%                 2.022000e+03               1.900000e+01   \n",
       "max                 2.023000e+03               2.300000e+01   \n",
       "\n",
       "       tpep_pickup_datetime_minute  ...  RatecodeID_4  RatecodeID_5  \\\n",
       "count                 3.760366e+07  ...  3.760366e+07  3.760366e+07   \n",
       "mean                  2.953681e+01  ...  8.343337e-04  6.012686e-03   \n",
       "std                   1.732159e+01  ...  2.887278e-02  7.730805e-02   \n",
       "min                   0.000000e+00  ...  0.000000e+00  0.000000e+00   \n",
       "25%                   1.400000e+01  ...  0.000000e+00  0.000000e+00   \n",
       "50%                   3.000000e+01  ...  0.000000e+00  0.000000e+00   \n",
       "75%                   4.500000e+01  ...  0.000000e+00  0.000000e+00   \n",
       "max                   5.900000e+01  ...  1.000000e+00  1.000000e+00   \n",
       "\n",
       "       RatecodeID_6  RatecodeID_99  payment_type_1  payment_type_2  \\\n",
       "count  3.760366e+07   3.760366e+07    3.760366e+07    3.760366e+07   \n",
       "mean   3.031620e-06   3.507611e-03    7.937295e-01    2.006221e-01   \n",
       "std    1.741152e-03   5.912113e-02    4.046269e-01    4.004658e-01   \n",
       "min    0.000000e+00   0.000000e+00    0.000000e+00    0.000000e+00   \n",
       "25%    0.000000e+00   0.000000e+00    1.000000e+00    0.000000e+00   \n",
       "50%    0.000000e+00   0.000000e+00    1.000000e+00    0.000000e+00   \n",
       "75%    0.000000e+00   0.000000e+00    1.000000e+00    0.000000e+00   \n",
       "max    1.000000e+00   1.000000e+00    1.000000e+00    1.000000e+00   \n",
       "\n",
       "       payment_type_3  payment_type_4  payment_type_5  total_amount  \n",
       "count    3.760366e+07    3.760366e+07    3.760366e+07  3.760366e+07  \n",
       "mean     3.205938e-03    2.442395e-03    2.659316e-08  2.118878e+01  \n",
       "std      5.653017e-02    4.936021e-02    1.630741e-04  1.534797e+01  \n",
       "min      0.000000e+00    0.000000e+00    0.000000e+00  5.800000e+00  \n",
       "25%      0.000000e+00    0.000000e+00    0.000000e+00  1.230000e+01  \n",
       "50%      0.000000e+00    0.000000e+00    0.000000e+00  1.596000e+01  \n",
       "75%      0.000000e+00    0.000000e+00    0.000000e+00  2.256000e+01  \n",
       "max      1.000000e+00    1.000000e+00    1.000000e+00  9.396000e+01  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "237559bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37603658, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d542f217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passenger_count                 False\n",
       "trip_distance                   False\n",
       "PULocationID                    False\n",
       "DOLocationID                    False\n",
       "extra                           False\n",
       "tpep_pickup_datetime_day        False\n",
       "tpep_pickup_datetime_month      False\n",
       "tpep_pickup_datetime_year       False\n",
       "tpep_pickup_datetime_hour       False\n",
       "tpep_pickup_datetime_minute     False\n",
       "tpep_pickup_datetime_second     False\n",
       "tpep_dropoff_datetime_day       False\n",
       "tpep_dropoff_datetime_month     False\n",
       "tpep_dropoff_datetime_year      False\n",
       "tpep_dropoff_datetime_hour      False\n",
       "tpep_dropoff_datetime_minute    False\n",
       "tpep_dropoff_datetime_second    False\n",
       "trip_duration                   False\n",
       "RatecodeID_1                    False\n",
       "RatecodeID_2                    False\n",
       "RatecodeID_3                    False\n",
       "RatecodeID_4                    False\n",
       "RatecodeID_5                    False\n",
       "RatecodeID_6                    False\n",
       "RatecodeID_99                   False\n",
       "payment_type_1                  False\n",
       "payment_type_2                  False\n",
       "payment_type_3                  False\n",
       "payment_type_4                  False\n",
       "payment_type_5                  False\n",
       "total_amount                    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() * 100 / len(df) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3399d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'extra', 'tpep_pickup_datetime_day', 'tpep_pickup_datetime_month', 'tpep_pickup_datetime_year', 'tpep_pickup_datetime_hour', 'tpep_pickup_datetime_minute', 'tpep_pickup_datetime_second', 'tpep_dropoff_datetime_day', 'tpep_dropoff_datetime_month', 'tpep_dropoff_datetime_year', 'tpep_dropoff_datetime_hour', 'tpep_dropoff_datetime_minute', 'tpep_dropoff_datetime_second', 'trip_duration', 'RatecodeID_1', 'RatecodeID_2', 'RatecodeID_3', 'RatecodeID_4', 'RatecodeID_5', 'RatecodeID_6', 'RatecodeID_99', 'payment_type_1', 'payment_type_2', 'payment_type_3', 'payment_type_4', 'payment_type_5']\n",
      "['PULocationID', 'DOLocationID', 'tpep_pickup_datetime_day', 'tpep_pickup_datetime_month', 'tpep_pickup_datetime_year', 'tpep_pickup_datetime_hour', 'tpep_pickup_datetime_minute', 'tpep_pickup_datetime_second', 'tpep_dropoff_datetime_day', 'tpep_dropoff_datetime_month', 'tpep_dropoff_datetime_year', 'tpep_dropoff_datetime_hour', 'tpep_dropoff_datetime_minute', 'tpep_dropoff_datetime_second', 'RatecodeID_1', 'RatecodeID_2', 'RatecodeID_3', 'RatecodeID_4', 'RatecodeID_5', 'RatecodeID_6', 'RatecodeID_99', 'payment_type_1', 'payment_type_2', 'payment_type_3', 'payment_type_4', 'payment_type_5']\n"
     ]
    }
   ],
   "source": [
    "# printing the feature columns and skip normalization columns for sanity check\n",
    "# and for also copying over to use in the normalization step in the main logic of the project\n",
    "print(feature_columns)\n",
    "print(skip_normalization_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00bec5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not df.isna().any().any(), \"NaNs remain in dataframe after preprocessing!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f0a76",
   "metadata": {},
   "source": [
    "### Publishing the processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a94b675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(df, \"../../data/processed/nytaxi2022_preprocessed_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.6",
   "language": "python",
   "name": "py313"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
