\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{hyperref}
\usepackage{listings}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}

\title{DSA 5208 Project 1 \\ Stochastic Gradient Descent for Neural Networks using MPI}
\author{Shreya Sriram, A0327236E}
\date{September 26, 2025}

\begin{document}

\maketitle

\section{Introduction}
\subsection{Problem Statement}
The problem statement is to use stochastic gradient descent to update the gradients of a one hidden layer neural network to minimize the training loss until it ceases to decrease any further. 

For a given dataset,
$$
\mathcal{D}=\left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^{N}, \quad x^{(i)}=\left(x_{1}^{(i)}, \ldots, x_{m}^{(i)}\right) \in \mathbb{R}^{m}, \quad y^{(i)} \in \mathbb{R}
$$

a neural network with one hidden layer approximates the map from $x_{i}$ to $y_{i}$ using the following equation:

$$
f(x ; \theta)=\sum_{j=1}^{n} w_{j} \sigma\left(\sum_{k=1}^{m} w_{j k} x_{k}+w_{j, m+1}\right)+w_{n+1}
$$

where

$$
\begin{aligned}
\theta=( & w_{1}, \ldots, w_{n}, w_{n+1} \\
& w_{11}, \ldots, w_{1, m+1} \\
& \ldots \\
& \left.w_{n 1}, \ldots, w_{n, m+1}\right)
\end{aligned}
$$

is the set of all parameters in the model and $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ is a non-linear activation function. The stochastic gradient method aims to solve the following minimization problem:

$$
\min _{\theta} R(\theta)=\frac{1}{2 N} \sum_{i=1}^{N}\left|f\left(x^{(i)} ; \theta\right)-y^{(i)}\right|^{2}
$$

To find the optimal value of $\theta$, we start with an initial guess $\theta_{0}$, and update the solution with


\begin{equation*}
\theta_{k+1}=\theta_{k}-\eta \widetilde{\nabla_{\theta} R}\left(\theta_{k}\right), \tag{1}
\end{equation*}


where $\eta$ is the learning rate, and $\widetilde{\nabla_{\theta} R}$ is the approximation of the gradient

$$
\nabla_{\theta} R(\theta)=\frac{1}{N} \sum_{i=1}^{N}\left[f\left(x^{(i)} ; \theta\right)-y^{(i)}\right] \nabla_{\theta} f\left(x^{(i)} ; \theta\right) .
$$

The approximation is done by randomly drawing $M$ distinct integers $\left\{j_{1}, \ldots, j_{M}\right\}$ from the set $\{1, \cdots, N\}$ and setting $\widetilde{\nabla_{\theta} R}$ to

$$
\widetilde{\nabla_{\theta} R}(\theta)=\frac{1}{M} \sum_{i=1}^{M}\left[f\left(x^{\left(j_{i}\right)} ; \theta\right)-y^{\left(j_{i}\right)}\right] \nabla_{\theta} f\left(x^{\left(j_{i}\right)} ; \theta\right)
$$

The random set $\left\{j_{1}, \ldots, j_{M}\right\}$ must be updated for every iteration (1). The iteration terminates when $R\left(\theta_{k}\right)$ no longer decreases.

\subsection{Dataset}
The dataset to test the solution to the problem statement on is nytaxi2022.csv which has $39656098$ rows and $19$ attributes. These attributes are described below (source: \href{https://www.kaggle.com/datasets/diishasiing/revenue-for-cab-drivers}{Kaggle}) :

\begin{enumerate}
    \item VendorID: A unique identifier for the taxi vendor or service provider.
    \item \verb|tpep_pickup_datetime|: \\ The date and time when the passenger was picked up.
    \item \verb|ttpep_dropoff_datetime|: The date and time when the passenger was dropped off.
    \item \verb|tpassenger_count|: The number of passengers in the taxi.
    \item \verb|ttrip_distance|: The total distance of the trip in miles or kilometers.
    \item \verb|tRatecodeID|: The rate code assigned to the trip, representing fare types.
    \item \verb|store_and_fwd_flag|: Indicates whether the trip data was stored locally and then forwarded later (Y/N).
    \item \verb|PULocationID|: The unique identifier for the pickup location (zone or area).
    \item \verb|DOLocationID|: The unique identifier for the drop-off location (zone or area). 
    \item \verb|payment_type|: The method of payment used by the passenger (e.g.,  cash, card).
    \item \verb|fare_amount|: The base fare for the trip.
    \item \verb|extra|: Additional charges applied during the trip (e.g., night surcharge).
    \item \verb|mta_tax|: The tax imposed by the Metropolitan Transportation Authority.
    \item \verb|tip_amount|: The tip given to the driver, if applicable.
    \item \verb|tolls_amount|: The total amount of tolls charged during the trip.
    \item \verb|improvement_surcharge|: A surcharge imposed for the improvement of services.
    \item \verb|total_amount|: The total fare amount, including all charges and surcharges.
    \item \verb|congestion_surcharge|: An additional charge for trips taken during high traffic congestion times.

\end{enumerate}

\subsection{Requirements}
\begin{itemize}
  \item The code should work for any number of processes.
  \item The dataset is stored nearly evenly among processes, and the algorithm should not send the local dataset to other processes, except when reading the data.
  \item Split the dataset into a training set (70\%) and a test set (30\%).
  \item Predict the total fare amount (column total\_amount) using the following columns as features: \\ \verb|tpep_pickup_datetime|, \verb|tpep_dropoff_datetime|, \verb|passenger_count|, \verb|trip_distance|, \verb|RatecodeID|, \verb|PULocationID|
    \verb| DOLocationID|, \verb |payment\_type|, \verb|extra|
  \item You may need to preprocess the data by dropping some incomplete rows and normalizing the data.

  \item All processes should compute the stochastic gradient $\widetilde{\nabla_{\theta} R}$ in parallel.
  \item Once the solution $\theta$ is found, the code can compute the RMSE in parallel.
\end{itemize}

\newpage
\section{Main Approach}
\subsection{Exploratory Data Analysis}
Since the given dataset was huge, to effectively process it locally on my machine in a distributed setting, I performed the following exploratory data analysis (not distributed) in a Jupyter notebook \verb|exploratory_data_analysis.ipynb| :

\begin{enumerate}
    \item I loaded the input file using the pandas' library's \verb|read_csv| function while parsing the datetime attributes \verb|tpep_pickup_datetime|, \\ 
    \verb|tpep_dropoff_datetime| the first time and cached 
    the entire dataframe as a pickle file to speed up subsequent runs.
    \item I checked the percentage of missing data in columns across the dataframe which was $3.450423$ \%, since the fraction was small, I chose to drop the rows with any missing data.
    \item I checked for duplicate rows to prevent bias during training. Since there was only 1 duplicate row, I dropped that row.
    \item I converted the pickup/dropoff datetime attributes to year, month, date, hour, minute, second and computed the trip duration in minutes between the pickup and dropoff times. I subsequently dropped the original datetime attributes as the derived attributes are sufficient to use for training and evaluation purposes.
    \item I dropped the rows where \verb|trip_duration| was either non-positive or over 3 hours to eliminate any noisy/invalid data.
    \item I dropped the rows with negative values of \verb|extra| and the non-positive values of \verb|total_amount|. As I further analyzed the range of values of \verb|total_amount|, I noticed extremely high values and hence chose to winsorize $0.5$ \% of the dataset i.e. clipped the \verb|total_amount| to be between $0.5$ percentile and $99.5$ percentile to be conservative with the removal of the outliers. This cleared up $0.9394557120998955$ \% of the dataset. 
    \item I checked the frequency counts of the categorical variables i.e. \verb|RatecodeID|, \verb|payment_type|, \verb|PULocationID|, \verb|DOLocationID| to determine the encoding to use for them. Since \verb|RatecodeID|, \verb|payment_type| only have 7 values and 6 values respectively, I chose to perform one-hot encoding for them. Since \verb|PULocationID|, \verb|DOLocationID| have 262 values each, one-hot encoding could significantly impact training times hence I chose frequency encoding for them to keep the dimensionality low.
    \item I checked the distribution of \verb|total_amount| which was right skewed as indicated below:

    \begin{center}
    \includegraphics[width=100mm,scale=0.5]{histogram-1.png }
    \end{center}

    So I performed a log transformation of the response variable (this was also done as a part of the invocation of \verb|final_checks()| method in same Jupyter notebook) the  so as to reduce skewness and reduce the impact of any outliers:

    \begin{center}
    \includegraphics[width=100mm,scale=0.5]{histogram-2.png }
    \end{center}
    
    \item I performed a final round of checks to ensure the data was clean before writing the data to another csv for modeling the Neural Net.
\end{enumerate} \\

The exploratory data analysis concluded with a reduction from $39656098$ rows to $37603658$ rows through removal of outliers, duplicates, invalid data along with the necessary transformation of given columns through datetime attribute extraction, encoding etc. to have $30$ features (excluding the response variable \verb|total_amount|)

\subsection{Distributed System}
The distributed system comprises the following key components that each execute in parallel across different processes:
\begin{itemize}
    \item IO and data splitting between train and test : Reading the processed input from the previous step and splitting the data between test and train 
    \item Normalizer: Normalizing the train features and labels, using their mean and stddev to normalize the test data
    \item Neural Network: Trains the model using Stochastic Gradient Descent for batch updates on the test data until the loss is within a threshold and evaluates the model using the test data. 
\end{itemize}

\subsubsection{IO and splitting the data between test and train}

The IO and test-train split can be summarized as follows: \\

Since one of the requirements is to store data nearly evenly among processes, the process with rank $0$ counts the number of rows in the input file and then broadcasts the count to the other processes using the MPI broadcast function:\\
    
\begin{code}
$\verb|num_rows_total| = comm.bcast(\verb|n_rows|, root=0)$       
\end{code} \\

Since each process is aware of the total number of rows, each process locally determines the number of rows it needs to read based on its rank. The start index, end index and skiprows are computed using the rank, and this makes the split among all processes nearly uniform. \\

To make the reading more efficient, we perform a chunk-based reading of the processed file with a chunk size of $100000$ \\
 
For each process, their local chunks are split into test and train - this is done by generating a permutation of indices using RNG and splitting the indices between test and train.

\subsubsection{Normalizer}
Within each process, the normalizer executes the standard scaling logic, i.e., centering the data to the global training mean and dividing by the global training stddev (non-zero).
The global training mean and global training stddev for the training features are computed using the MPI allreduce function:\\

$\verb|feature_fields |= comm.allreduce((\verb|local_feature_sum, local_feature_count|), op=MPI.SUM)$ \\
$\verb|global_feature_sum|, \verb|global_feature_count| = \verb|feature_fields[0]|, \verb|feature_fields[1]|$ \\

The global training mean and global training stddev for the training labels are computed in a similar manner. \\

These means and stddevs are used to normalize the testing data. \\

Since one may choose not to normalize certain attributes, I added the ability to skip normalization for certain columns to evaluate the impact of doing so on the experiments. \\

\subsubsection{Neural Network}
The One Hidden Layer Neural Network has the following steps that are applied in the training stage:
\begin{enumerate}
    \item Forward propagation: Starting off with random weights, the input data (i.e. features from the processed data) is passed on to a hidden layer which then generates an output. So the activation is calculated on the weighted sum of inputs which in turn is used in another weighted summation to yield a prediction of the output
    \item Backward propagation: The predicted output generated by forward propagation is then compared against the actual output to adjust the weights and the bias at the hidden layer and then backpropagate it to adjust the weights and bias at the input layer using derivative of the chosen activation function. 
    \item Gradient update: The weights are adjusted based on the applicable learning rate and gradients. The gradients are computed at a batch level (using M integers drawn at random), they are then calculated using \\ \verb|global_summed_gradients| and \verb|global_count|, obtained by MPI's allreduce function on local weighted gradients (weighted as the sizes of batches may vary) and local count respectively.
\end{enumerate} \\

These steps are continuously repeated within a batch until the difference between the loss computed for a batch of data and its previous batch are within the stopping criterion (I tried $1e-5$ or $1e-6$ as the stopping criterion threshold in the result/attempts in the subsequent sections) or if the number of iterations are over the \verb|max_iterations| input by the user. This approximation of global loss at the batch level is used as a stopping criterion to exit training. The train RMSE is calculated using the updated weights/bias on the full train dataset.

The evaluation stage uses the weights computed in the training stage to predict the labels, the test RMSE is computed similar to how the train RMSE is computed. 

Additional notes:

\begin{enumerate}
    \item Weight initialization: The starting weights are initialized based on the activation function to ensure faster
    convergence following (Reference: \\ \href{https://www.deeplearning.ai/ai-notes/initialization/index.html}{DeepLearning.AI Weight Initialization})
    \item Learning rate initialization: I tried three types of learning rates - a constant learning rate, a cyclic scheduler learning rate (Reference: \href{https://machinelearningmastery.com/a-gentle-introduction-to-learning-rate-schedulers/}{Learning Rate Schedulers}) and a modified cyclic learning rate (that uses number of processes as a factor), details of the results for each of the three have been documented in the next two sections.
\end{enumerate}

\section{Results}
The results from the One Hidden Layer Neural Network model using the constant learning rate indicate that the training model generalizes well on the test data as training RMSE and testing RMSE are nearly equal across the board. The plot below is a scatterplot between train RMSE and test RMSE for the 5 activation functions I tried (\verb|relu, tanh, sigmoid,| \\ \verb|leaky_relu, elu|) and by the number of processes (1 through 5) - the datapoints in each of the subplots correspond to the batch sizes as indicated in the legend of the grid plot.

The parameters used to train this model are : \\
(1) nodes in the hidden layer = $16$ as about half the size of the input features ($30$) (Reference: \href{https://medium.com/data-science/17-rules-of-thumb-for-building-a-neural-network-93356f9930af}{Medium article - Building a Neural Network}), \\
(2) constant learning rate of $1e-5$, \\
(3) Stopping criterion: stopping threshold of $1e-5$ for the difference in consecutive batch losses or a max number of iterations of 1000000 \\

\begin{center}
\includegraphics[width=100mm,scale=0.5]{constant-image1.png }
\end{center}

However, the constant learning rate results in early convergence resulting in high RMSEs. The table below summarizes the best performing configurations (throughout this report, configurations are stated as \\ \verb|num_processes - batch_size - activation|) for constant learning rate (NOTE: The Train Time is in minutes unless noted otherwise) :

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Configuration & Iterations & Learning Rate & Train RMSE & Test RMSE & Max Train Time \\
\hline
4 - 16 - \verb|tanh| & 109129 & 1e-05 & 0.449943 & 0.449951 & 2.78 \\
3 - 16 - \verb|elu| & 120067 & 1e-05 & 0.475215 & 0.474462 & 3.79 \\
5 - 16 - \verb|sigmoid| & 68090 & 1e-05 & 0.478182 & 0.478231 & 1.44 \\
4 - 16 - \verb|leaky_relu| & 138578  & 1e-05 & 0.507784 & 0.506763 & 4.11 \\
5 - 32 - \verb|tanh| & 48998 & 1e-05 & 0.518215 & 0.518242 & 1.05 \\
\hline
\end{tabular}
\end{center}

The constant learning rate demonstrates consistent train-test generalization but suffers from early convergence, with the best test RMSE around $0.449951$ which is high and suggests room for improvement.

The timing plots show interesting patterns in distributed performance. The gap between maximum and average training times reflects MPI synchronization overhead, where the slowest process determines when all processes can proceed. Generally, the max/avg time ratios remain fairly consistent, suggesting that the data is being partitioned effectively across processes. The spikes/variations could also be attributed to the fact that I was running this code on my local machine. Since my machine has resource constraints and has numerous other processes running, the training times may not be consistent upon repeating a run with the same configurations. 

Some timing spikes appear at a higher number of processes, likely when communication overhead starts outweighing computational benefits. 
Larger batch sizes tend to perform better in parallel because they increase the computation-to-communication ratio and have lower MPI overhead.

NOTE: The train times indicated in the plots and the tables in the report are all in minutes.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{constant-image2.png }
\end{center}

\begin{center}
\includegraphics[width=90mm,scale=0.5]{constant-image3.png }
\end{center}

Looking at the training loss curves, there's a clear difference between single-process and multi-process runs. With just one process, the loss shows more spikes and instability, which makes sense since there's no gradient averaging happening. When using multiple processes, the loss curves become much smoother because gradients are being averaged across processes.

The constant learning rate enables fast convergence, but this speed comes at a cost - the model likely stops improving before finding the best solution.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{constant-image4.png }
\end{center}

The learning rate is constant as the name suggests, and the same is indicated by the plot below.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{constant-image5.png }
\end{center}

\section{Attempts to improve test RMSE and convergence}

\subsection {Attempt 1: Cyclic Scheduler Learning Rate }
In an attempt to slow down convergence and have lower RMSE values, I tried using cyclic scheduler with a base learning rate of $1e-5$, step size varying with batch size and max learning rate of $2e-4$ i.e. the learning rate would keep cycling between the base learning rate and max learning rate enabling gradient adjustment in a suitable way over multiple (cycles of) iterations.

The parameters used to train this model are : \\
(1) nodes in the hidden layer = $16$ as about half the size of the input features ($30$) (Reference: \href{https://medium.com/data-science/17-rules-of-thumb-for-building-a-neural-network-93356f9930af}{Medium article - Building a Neural Network}), \\
(2) cyclical learning rate between $1e-5$ and $2e-4$, \\
(3) Stopping criterion: stopping threshold of $1e-5$ for the difference in consecutive batch losses or a max number of iterations of 1000000 \\

\begin{center}
\includegraphics[width=90mm,scale=0.5]{cyclic-image1.png }
\end{center}

There is a visible improvement in the RMSE values compared to the constant learning rate. The table below shows the top performing configurations \\ \verb|num_processes - batch_size - activation|) for standard cyclic learning rate:

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Configuration & Iterations & Learning Rate & Train RMSE & Test RMSE & Max Train Time \\
\hline
5 - 64 - \verb|tanh|  & 98349 & 0.000135 & 0.209547 & 0.209554 & 1.44 \\
5 - 16 - \verb|elu| & 112690 & 0.000185 & 0.219785 & 0.21937 & 1.66 \\
5 - 32 - \verb|elu|  & 91922 & 0.000156 & 0.229462 & 0.228855 & 1.46 \\
4 - 64 - \verb|elu|  & 93971 & 0.000088 & 0.244717 & 0.243665 & 1.19 \\
3 - 16 - \verb|elu|  & 152924 & 0.000168 & 0.258685 & 0.257498 & 1.78 \\

\hline
\end{tabular}
\end{center}

The cyclic learning rate shows substantial improvement over constant learning rate, with the best result of 0.209554 RMSE. However, training times are significantly longer due to the cycling nature preventing early convergence.

The cyclic learning rate creates more complex timing patterns than the constant approach. Some specific combinations work well for parallelization - for instance, ReLU with batch sizes 128 or 512, and sigmoid/leaky ReLU with batch size 256. The timing isn't always predictable and sometimes adding more processes doesn't speed things up as much as expected.

One possible reason is the cyclic nature of the learning rate that adds computational overhead which varies depending on the activation function used. The training takes longer compared to constant learning rate, but the trade-off is worth it given the improvement in the test RMSE values.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{cyclic-image2.png }
\end{center}

\begin{center}
\includegraphics[width=90mm,scale=0.5]{cyclic-image3.png }
\end{center}

Compared to the line chart for constant learning rate, the following line plot suggests that the cyclical scheduler is enabling the model to quickly switch back from large spikes i.e., there is higher loss due to the cyclic nature of the learning rate. A slower convergence rate can be seen for  certain permutations of parameters \verb|(activation_type, num_processes)| - \\ \verb|((tanh,2 ), (leaky_relu, 2), (elu, 3), (elu, 4), (elu, 5))|

\begin{center}
\includegraphics[width=90mm,scale=0.5]{cyclic-image4.png }
\end{center}

The learning rate is cyclic as the name suggests, with an adjustment by batch size and the same is indicated by the plot below.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{cyclic-image5.png }
\end{center}

\subsection {Attempt 2: Custom/Hybrid Learning Rate }
This attempt was to purely experiment with the learning rate to see if a custom function using a cyclic function with an additional factor of the number of processes could assist with having even lower RMSE values while also enabling more effective use of parallel processes; I modified the cyclic scheduler from the previous attempt to include a factor of size (size being the number of processes in the MPI notation).

The parameters used to train this model are : \\
(1) nodes in the hidden layer = 16 as about half the size of the input features (30) (Reference: \href{https://medium.com/data-science/17-rules-of-thumb-for-building-a-neural-network-93356f9930af}{Medium article - Building a Neural Network}), \\
(2) cyclical learning rate that cycles based on process count and batch size with base learning rate 1e-5 and varying max learning rate, \\
(3) Stopping criterion: stopping threshold of $1e-5$ for the difference in consecutive batch losses or a max number of iterations of 1000000

\begin{center}
\includegraphics[width=90mm,scale=0.5]{hybrid-image1.png }
\end{center}

There is further improvement in the RMSE values compared to the previous two attempts specifically when there are multiple processes at play - \verb|tanh, leaky_relu, relu, elu| have some of the lowest RMSEs when we train/test the model across batches of all different sizes. The table below demonstrates the performance of the size-factor enhanced learning rate:

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Configuration & Iterations & Learning Rate & Train RMSE & Test RMSE & Max Train Time \\
\hline

5 - 16 - \verb |tanh| & 67016 & 0.000776 & 0.183704 & 0.183761 & 1.0 \\
5 - 16 - \verb |relu| & 128625 & 0.0005325 & 0.192592 & 0.191145 & 2.84 \\
5 - 32 - \verb |tanh| & 33364 & 0.00063168 & 0.194881 & 0.194917 & 0.8 \\
4 - 128 - \verb |tanh| & 74554 & 0.00055948 & 0.201903 & 0.201929 & 1.36 \\
5 - 16 - \verb |leaky_relu| & 67106 & 0.000810 & 0.207769 & 0.207469 & 1.4 \\

\hline
\end{tabular}
\end{center}

The size-factor modification is an improvement over standard cyclic learning rate (0.183704 vs 0.209554 RMSE). Most remarkably, the best configuration continues to demonstrate perfect generalization with near-identical train and test RMSE.

By scaling the learning rate with the number of processes, this approach helps compensate for how gradients are averaged across processes in distributed training. While training takes longer than previous two approaches, the quality of results i.e., achieving 0.1837 RMSE with the 1e-5 stopping criterion, compensates for the extra time.

Different activation functions show varying timing patterns under this approach, which likely reflects their different computational requirements and how they respond to the scaled learning rate.

This approach prioritizes achieving the best possible model quality against unseen data over training time, it achieves this better than the previous two approaches.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{hybrid-image2.png }
\end{center}

\begin{center}
\includegraphics[width=90mm,scale=0.5]{hybrid-image3.png }
\end{center}

With the inclusion of the size-factor modification, the learning rate demonstrates improved stability and convergence as the number of processes increases.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{hybrid-image4.png}
\end{center}

The learning rate is still essentially cyclic, it fluctuates with both the batch size and the number of processes is indicated in the plot below.

\begin{center}
\includegraphics[width=90mm,scale=0.5]{hybrid-image5.png }
\end{center}

\subsection{Attempt 3: Custom Learning Rate with stricter stopping criteria}

I updated the stopping criteria to be $1e-6$ (from $1e-5$) to slow down the convergence while attempting to improve the test RMSE. Owing to the performance of \verb|tanh, relu, leaky_relu, elu| for process count = 5 and batch sizes = 16/32 in Attempt 2, I limited testing this improvement attempt to these cases. This attempt achieved the best results:

The parameters used to train this model are exactly the same as that of Attempt 2 except for the updated stopping criterion noted above: \\
(1) nodes in the hidden layer = $16$ as about half the size of the input features ($30$) (Reference: \href{https://medium.com/data-science/17-rules-of-thumb-for-building-a-neural-network-93356f9930af}{Medium article - Building a Neural Network}), \\
(2) cyclical learning rate that cycles based on process count and batch size with base learning rate $1e-6$ and varying max learning rate, \\
(3) Stopping criterion: stopping threshold of $1e-6$ for the difference in consecutive batch losses or a max number of iterations of 1000000 \\

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Configuration & Iterations & Train RMSE & Test RMSE & Max Train Time \\
\hline
5 - 16 - \verb|tanh| & 851280 & 0.140719 & 0.140739 & 31.77 \\
5 - 16 - \verb|leaky_relu| & 1000000 & 0.14712 & 0.146933 & 28.8 \\
5 - 16 - \verb|elu| & 749295 & 0.153278 & 0.148911 & 17.14 \\
5 - 32 - \verb|elu| & 154711 & 0.182923 & 0.183267 & 4.5 \\
5 - 32 - \verb|tanh| & 33364 & 0.194881 & 0.194917 & 1.44 \\
\hline
\end{tabular}
\end{center}

The stricter stopping criterion ($1e-6$) produced even better results, with the best configuration achieving $0.1407$ RMSE and a near perfect train-test generalization. The test RMSE has improved by 23.42\% over the $1e-5$ results however the run took about 31x the time taken by the best results obtained using the $1e-5$ stopping threshold.

\subsection{Cross-Method Performance Comparison}
The evolution from constant to size-factor enhanced learning rates with stricter stopping criteria shows a significant improvement:

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Learning Rate & Test RMSE & Improvement & Max Train Time\\
\hline
Constant LR (1e-5) & 0.449951 & Baseline & 2.78 \\
Cyclic LR (1e-5) & 0.209554 & 53.4\% better & 1.44 \\
Size-Factor LR (1e-5) & 0.183761 & 59.2\% better & 1.0 \\
\textbf{Size-Factor LR (1e-6)} & \textbf{0.140719} & \textbf{68.7\% better} & \textbf{31.77} \\
\hline
\end{tabular}
\end{center}

The stricter stopping criterion combined with size-factor scaling achieved the best results in terms of the test RMSE however, the training time was substantially higher, so depending on the preference for a lower RMSE vs speed, either Approach 2 (faster, higher RMSE) or Approach 3 (slower, lower RMSE) can be chosen unless a more suitable approach is found.

\subsection{Key Learning: Size-Factor Learning Rate}
The main improvement compared to the constant learning rate came from scaling the learning rate by both the batch size and the number of processes (reference to Approach 2/3). When multiple processes average their gradients together, the gradient values could become smaller, the compensation by the factor of number of processes keeps the gradient updates suitable enough to help the model learn to generate low test RMSEs.\\

\textbf{Formula used for the effective learning rate (noted as "lr" below)}:
\begin{lstlisting}
 cycle = np.floor(1 + iteration / (2 * step_size))
 x = np.abs(iteration / step_size - 2 * cycle + 1)
 lr = base_lr + (max_lr - base_lr) * np.maximum(0, 1 - x) * num_processes
\end{lstlisting} \\

The inferences from this attempt are as follows :
\begin{enumerate}
    \item Much better RMSE results (68.7\% improvement over baseline constant learning rate)
    \item Train and test RMSE almost identical (good generalization)  
    \item Worked well with different activation functions
    \item Made better use of multiple processes
\end{enumerate}

\subsection{Batch Size Observations}
I noticed an important pattern with different batch sizes:

\begin{itemize}
    \item \textbf{Small batches (16)}: More randomness in training but better final results
    \item \textbf{Larger batches (32+)}: Smoother training but got stuck in worse solutions
    \item \textbf{Best choice}: Batch size $16$ gave the best balance of training time and accuracy
\end{itemize}

\section{Conclusion}

This report explores several approaches to implement distributed neural network training using MPI, achieving a good performance on the NYC taxi fare prediction dataset. The main experiment performed in this report is a cyclical learning rate that scales with the number of processes, helping to compensate for how gradients are averaged across distributed processes. Combined with a stricter convergence criterion ($1e-6$), this method achieved a test RMSE of 0.1407 with perfect train-test generalization - a $68.7$\% improvement over the constant learning rate baseline.

\section{Future Improvements}

If I had more time to work on this project, I would explore :

\subsection{Testing Different Approaches}
\begin{enumerate}
    \item Try other values for the number of nodes in the hidden layer like 75\% of input features or $2n+1$ hidden layer nodes where $n$ is the number of input layer nodes (Reference: \href{https://www.sciencedirect.com/topics/engineering/hidden-layer-node}{Science Direct Topics - Hidden layer node})
    \item Experiment with other learning rate schedulers
    \item Try other ways to scale the learning rate (maybe square root of processes instead of linear)
    \item Experiment with other activation functions like Swish or GELU that I did not try in this project
    \item Test the size-factor method on other datasets to see if it works generally
\end{enumerate}

\subsection{Better Performance}
\begin{enumerate}
    \item Test with more than 5 processes using virtual machines on a platform like Google Cloud to see how well it could scale in a Production setting. Additionally, this would isolate the impact of other processes running on my machine, which may have caused spiky/increased training times for certain runs (as also noted in the Results section). 
    \item Find ways to handle even larger datasets that don't fit in memory
\end{enumerate}

\subsection{Code Improvements}
\begin{enumerate}
    \item Track more granular timing metrics to identify bottlenecks
    \item Structure the code in a way that it can be easily reused with other ML algorithms or other gradient updation methods
    \item Add cross-validation to evaluate a good model instead of just computing train and test RMSE since the same hyperparameters could perform poorly on another unknown dataset.
\end{enumerate}

\end{document}